<pl-answer-panel>
<p>In the case study on "Deep Neural Nets: 33 years ago and 33 years from now", which of the following techniques did we try that improved the ability of the model to generalize (predict on data not used in training)?</p>
</pl-answer-panel>

<pl-hide-in-panel answer="true">

<pl-checkbox number-answers="3" answers-name="tf" weight="1" hide-letter-keys="true">
  <pl-answer correct="true">Increase the size of the training data set</pl-answer>
  <pl-answer correct="true">Shift each image up to one pixel in each dimension</pl-answer>
  <pl-answer correct="true">Add a layer with 0.25 dropout probability</pl-answer>
  <pl-answer>Increase the number of filters in each of the convolutional layers</pl-answer>
  <pl-answer>Increase the number of hidden layers in the network</pl-answer>
  <pl-answer>Decrease the depth and width of the network to avoid overfitting</pl-answer>
  <pl-answer>Decrease the model complexity to avoid overfitting</pl-answer>
</pl-checkbox>

</pl-hide-in-panel>
