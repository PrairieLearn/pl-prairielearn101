<pl-question-panel>
  <p>The following are ways to reduce prediction serving latency. Match each to approach to a disadvantage of that approach.</p>
</pl-question-panel>

<pl-matching answers-name="string_value">
  <pl-statement match="Makes computation more costly">Use hardware acceleration e.g. GPU rather than CPU</pl-statement>
  <pl-statement match="May have worse error performance">Use a smaller model, or reduce the size of a big model (e.g. by pruning or knowledge distillation)</pl-statement>
  <pl-statement match="Not ideal for model privacy">Run inference on an edge device, rather than in a cloud</pl-statement>
  <pl-option>May be affected by the performance of the communication network over which data and results are transmitted</pl-option>
  <pl-option>Less preferred for user data privacy</pl-option>
</pl-matching>
